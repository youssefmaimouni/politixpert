import pandas as pd
import numpy as np
import faiss
import torch
import pickle
import os
import re
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from collections import defaultdict

class PolitixpertRAG:
    def __init__(self, csv_path):
        print("ğŸ”„ Initialisation du moteur RAG (OptimisÃ© CPU + Local)...")
        self.device = "cpu"
        
        # Chemins des fichiers
        self.emb_file = "embeddings.npy"
        self.meta_file = "metadata.pkl"
        
        # Chemins des modÃ¨les locaux (tÃ©lÃ©chargÃ©s via download_models.py)
        # Si les dossiers n'existent pas, changez pour les IDs HuggingFace
        self.model_path_e5 = "./models/e5" if os.path.exists("./models/e5") else "intfloat/multilingual-e5-base"
        self.model_path_qwen = "./models/qwen" if os.path.exists("./models/qwen") else "Qwen/Qwen2.5-1.5B-Instruct"

        # 1. VÃ©rification si les fichiers Kaggle existent
        if os.path.exists(self.emb_file) and os.path.exists(self.meta_file):
            print("ğŸš€ Fichiers prÃ©-calculÃ©s trouvÃ©s ! Chargement rapide...")
            self._load_precomputed()
        else:
            print("âš ï¸ Fichiers prÃ©-calculÃ©s introuvables. Calcul local (LENT)...")
            self.df = pd.read_csv(csv_path)
            self.df = self.df[self.df["content"].notna()]
            self.df = self.df[self.df["content"].str.len() > 100].reset_index(drop=True)
            self.chunks = []
            self.metadata = []
            self._prepare_chunks()
            
            self.embed_model = SentenceTransformer(self.model_path_e5, device="cpu")
            self._compute_index_locally()

        # On charge le modÃ¨le d'embedding s'il n'est pas dÃ©jÃ  chargÃ©
        if not hasattr(self, 'embed_model'):
             print(f"ğŸ§  Chargement du modÃ¨le d'embedding depuis {self.model_path_e5}...")
             self.embed_model = SentenceTransformer(self.model_path_e5, device="cpu")

        # 5. Chargement du LLM (Qwen) sur CPU
        print(f"ğŸ¤– Chargement du LLM (Qwen) depuis {self.model_path_qwen}...")
        self.generator = pipeline(
            "text-generation",
            model=self.model_path_qwen,
            device=-1, # CPU
            trust_remote_code=True,
            model_kwargs={"low_cpu_mem_usage": True} # Optimisation RAM
        )
        print("âœ… SystÃ¨me prÃªt !")

    def _load_precomputed(self):
        embeddings = np.load(self.emb_file).astype("float32")
        with open(self.meta_file, "rb") as f:
            self.metadata = pickle.load(f)
        self.chunks = [m["text"] for m in self.metadata]
        
        print(f"ğŸ—‚ï¸ Indexation de {len(embeddings)} vecteurs...")
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dim)
        self.index.add(embeddings)

    def _chunk_text(self, text, chunk_size=300, overlap=50):
        words = text.split()
        chunks = []
        start = 0
        while start < len(words):
            end = start + chunk_size
            chunk = " ".join(words[start:end])
            chunks.append(chunk)
            start += chunk_size - overlap
        return chunks

    def _prepare_chunks(self):
        for idx, row in self.df.iterrows():
            text_chunks = self._chunk_text(row["content"])
            for i, chunk in enumerate(text_chunks):
                self.chunks.append(chunk)
                self.metadata.append({
                    "title": row.get("title", "Sans titre"),
                    "description": row.get("description", ""),
                    "date": str(row.get("date", "")),
                    "source": row["source"],
                    "link": row.get("link", "#"),
                    "text": chunk
                })

    def _compute_index_locally(self):
        embeddings = self.embed_model.encode(
            self.chunks, 
            batch_size=16, 
            show_progress_bar=True, 
            normalize_embeddings=True
        )
        embeddings = np.array(embeddings).astype("float32")
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dim)
        self.index.add(embeddings)

    def _clean_output(self, text):
        """Supprime les caractÃ¨res chinois et nettoie le texte"""
        text = re.sub(r'[\u4e00-\u9fff]+', '', text) # Supprime Hanzi
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def semantic_search(self, query, top_k=20, min_score=0.82):
        """Recherche avec filtrage strict (Score & Longueur)"""
        q_emb = self.embed_model.encode([query], normalize_embeddings=True).astype("float32")
        
        # On cherche 2x plus de candidats pour pouvoir filtrer
        search_k = top_k * 2
        scores, indices = self.index.search(q_emb, search_k)
        
        results = []
        for i in range(search_k):
            score = float(scores[0][i])
            idx = indices[0][i]
            
            # FILTRE 1 : Score de pertinence
            if score < min_score:
                continue

            if idx < len(self.metadata):
                meta = self.metadata[idx]
                text_content = meta.get("text", self.chunks[idx])
                
                # FILTRE 2 : Longueur du texte (Ã©viter le bruit)
                if len(text_content) < 50:
                    continue
                
                results.append({
                    "score": score,
                    "text": text_content,
                    "title": meta["title"],
                    "description": meta["description"],
                    "date": meta["date"],
                    "source": meta["source"],
                    "link": meta["link"]
                })
                
                if len(results) >= top_k:
                    break
        return results

    def _build_context(self, docs, max_docs=4):
        context_parts = []
        sources = []
        
        for d in docs[:max_docs]:
            # Contexte enrichi avec la DATE et la DESCRIPTION
            doc_entry = f"""
Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©:
- Ø§Ù„ØªØ§Ø±ÙŠØ®: {d['date']}
- Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {d['title']}
- Ø§Ù„Ø³ÙŠØ§Ù‚: {d['description']}
- Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ: {d['text']}
"""
            context_parts.append(doc_entry)
            sources.append({"title": d['title'], "date": d['date'], "link": d['link']})
            
        return "\n___________________\n".join(context_parts), sources

    def generate_answer(self, question):
        # On demande 20 docs, et on filtre avec min_score=0.82
        results = self.semantic_search(question, top_k=20, min_score=0.82)
        
        grouped = defaultdict(list)
        for r in results:
            grouped[r["source"]].append(r)

        final_response = []

        for party, docs in grouped.items():
            context_str, sources = self._build_context(docs)
            
            # Prompt ROBURSTE (Dates + Espace + Arabe uniquement)
            messages = [
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø³ÙŠØ§Ø³ÙŠ Ø®Ø¨ÙŠØ±. Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·."},
                {"role": "user", "content": f"""
Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ù„Ø®Øµ Ù…ÙˆÙ‚Ù Ø­Ø²Ø¨ "{party}" Ø¨Ø®ØµÙˆØµ Ø§Ù„Ø³Ø¤Ø§Ù„: "{question}".

Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ§Ø­Ø©:
{context_str}

âš ï¸ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø© (Strict Instructions):
1. **Ø§Ù„Ù„ØºØ©**: Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· (Arabic Only). Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£Ø­Ø±Ù ØµÙŠÙ†ÙŠØ© Ø£Ùˆ Ø±Ù…ÙˆØ² ØºØ±ÙŠØ¨Ø©.
2. **Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®**: Ø§Ù†ØªØ¨Ù‡ Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù…Ø«Ù„Ø§Ù‹ Ù†ØµÙˆØµ Ù†Ù‡Ø§ÙŠØ© 2023 ØªØªØ­Ø¯Ø« Ø¹Ù† Ù…Ø§Ù„ÙŠØ© 2024).
3. **Ø§Ù„ÙØ¶Ø§Ø¡**: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† "ØºØ²Ùˆ Ø§Ù„ÙØ¶Ø§Ø¡" Ø£Ùˆ "Ø§Ù„ÙƒÙˆØ§ÙƒØ¨" ÙˆØ§Ù„Ù†ØµÙˆØµ Ø³ÙŠØ§Ø³ÙŠØ©ØŒ Ø§ÙƒØªØ¨ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª".
4. **Ø§Ù„ØªÙ„Ø®ÙŠØµ**: Ù„Ø®Øµ Ø§Ù„Ù…ÙˆÙ‚Ù ÙÙŠ ÙÙ‚Ø±Ø© Ø£Ùˆ ÙÙ‚Ø±ØªÙŠÙ† Ø¨ØµÙŠØºØ© Ø§Ù„ØºØ§Ø¦Ø¨ (ÙŠØ±Ù‰ Ø§Ù„Ø­Ø²Ø¨ØŒ ÙŠØ¤ÙƒØ¯ Ø§Ù„Ø­Ø²Ø¨).
"""}
            ]

            try:
                out = self.generator(
                    messages, 
                    max_new_tokens=300, 
                    do_sample=False, 
                    return_full_text=False
                )
                raw_summary = out[0]["generated_text"]
                # Nettoyage final des caractÃ¨res chinois
                summary = self._clean_output(raw_summary)
            except Exception as e:
                print(f"Erreur gÃ©nÃ©ration pour {party}: {e}")
                summary = "ØªØ¹Ø°Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ."

            final_response.append({
                "party": party,
                "summary": summary,
                "sources": sources
            })
            
        return final_response